{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aeba5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python_code\\NLPGenAIcourse\\LangGraph\\resume_sel\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import List, Dict, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ChatMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate, ChatMessagePromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "\n",
    "#groq\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "generator_llm = ChatGroq(model =\"openai/gpt-oss-120b\",groq_api_key=groq_api_key)\n",
    "evaluator_llm = ChatGroq(model =\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "\n",
    "from pydantic import BaseModel, Field, conint\n",
    "from typing import List\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c01fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "class Evaluation(BaseModel):\n",
    "    \"\"\"A structured evaluation of the user's interview answer.\"\"\"\n",
    "    strengths: List[str] = Field(\n",
    "        description=\"A list of 2-3 bullet points highlighting what the user did well.\"\n",
    "    )\n",
    "    areas_for_improvement: List[str] = Field(\n",
    "        description=\"A list of 2-3 actionable bullet points for what could be improved.\"\n",
    "    )\n",
    "    overall_score: conint(ge=1, le=10) = Field(\n",
    "        description=\"A single integer score from 1 (poor) to 10 (excellent).\"\n",
    "    )\n",
    "    score_justification: str = Field(\n",
    "        description=\"A concise, one-sentence justification for the given score.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    job_role: str # User input\n",
    "    job_role_context: str # from research node\n",
    "    candidate_data: List[Dict[str, List[str]]] #parsed from candidate resume\n",
    "    ideal_answers: List[str]  # generated ideal answers\n",
    "    generated_qas: List[Dict[str, str]]   # [{question, answer}]\n",
    "    evaluation_feedback: List[Dict] # from evaluator node\n",
    "    current_q_index: int # which question we're on\n",
    "    wants_ideal: bool # whether the user wants ideal answers\n",
    "    num_steps: int # how many steps taken\n",
    "    max_iterations: int # max allowed steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fdf270",
   "metadata": {},
   "source": [
    "### NODES -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49250ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generator_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Uses the LLM (generator_llm) to create interview questions\n",
    "    (without answers) based on the job role context gathered earlier.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- ðŸ¤– GENERATING INTERVIEW QUESTIONS ONLY ---\")\n",
    "\n",
    "    # Ensure we have context\n",
    "    job_context = state.get(\"job_role_context\", \"\")\n",
    "    job_role = state.get(\"job_role\", \"\")\n",
    "\n",
    "    if not job_context:\n",
    "        raise ValueError(\"No job_role_context found. Research node must run first.\")\n",
    "\n",
    "    # Prompt for the LLM\n",
    "    query = f\"\"\"\n",
    "    You are an expert interviewer.\n",
    "    Generate exactly 5 unique interview questions for the role '{job_role}'.\n",
    "    Do NOT provide answers. \n",
    "    Respond in strict JSON format as a list of objects, \n",
    "    each with keys: \"question\" and \"answer\". \n",
    "    Leave \"answer\" as an empty string.\n",
    "    Example:\n",
    "    [\n",
    "      {{\"question\": \"What is X?\", \"answer\": \"\"}},\n",
    "      {{\"question\": \"How would you handle Y?\", \"answer\": \"\"}}\n",
    "    ]\n",
    "\n",
    "    Context:\\n\\n{job_context}\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the LLM\n",
    "    try:\n",
    "        response = generator_llm.invoke([HumanMessage(content=query)])\n",
    "        output_text = response.content\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error calling generator_llm: {e}\")\n",
    "        output_text = \"[]\"\n",
    "\n",
    "    # Try parsing into JSON\n",
    "    import json\n",
    "    try:\n",
    "        qas = json.loads(output_text)\n",
    "    except Exception:\n",
    "        print(\"[!] Model did not return valid JSON, falling back...\")\n",
    "        qas = [{\"question\": output_text, \"answer\": \"\"}]\n",
    "\n",
    "    # Save into state\n",
    "    state[\"generated_qas\"] = qas\n",
    "    state[\"num_steps\"] = state.get(\"num_steps\", 0) + 1\n",
    "\n",
    "    print(\"--- âœ… QUESTION GENERATION COMPLETE ---\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58f01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def research_node(state: GraphState) -> GraphState:\n",
    "\n",
    "    \"\"\"\n",
    "    Performs targeted web research on a given job role to gather context using the Tavily client.\n",
    "    \n",
    "    This node's single responsibility is to collect general information about the job role,\n",
    "    which will be used by downstream nodes.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"--- ðŸ” GATHERING JOB ROLE CONTEXT (Tavily Search) ---\")\n",
    "\n",
    "    # 1. Extract the job role from the state\n",
    "    job_role = state['job_role']\n",
    "    if not job_role:\n",
    "        raise ValueError(\"Job role cannot be empty.\")\n",
    "\n",
    "    # 2. Initialize the Tavily client\n",
    "    # Note: Ensure the TAVILY_API_KEY environment variable is set.\n",
    "    try:\n",
    "        tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "        if not tavily_api_key:\n",
    "            raise ValueError(\"TAVILY_API_KEY environment variable is not set.\")\n",
    "\n",
    "        tavily_client = TavilyClient(tavily_api_key)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Tavily client: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 3. Define a set of focused queries to build a comprehensive profile of the job role\n",
    "    search_queries = [\n",
    "        f\"key responsibilities and daily tasks of a {job_role}\",\n",
    "        f\"essential technical skills and required software tools for a {job_role} in 2025\",\n",
    "        f\"common educational background and certifications for a {job_role}\",\n",
    "        f\"latest industry trends and future outlook for the {job_role} career path\"\n",
    "    ]\n",
    "\n",
    "    print(f\"Performing {len(search_queries)} targeted searches for the role: '{job_role}'\")\n",
    "    \n",
    "    # 4. Execute searches and aggregate the content from the results\n",
    "    all_context = []\n",
    "    try:\n",
    "        for query in search_queries:\n",
    "            response = tavily_client.search(\n",
    "            query=query,\n",
    "            include_answer=\"advanced\",\n",
    "            search_depth=\"advanced\",\n",
    "            country=\"india\"\n",
    "        )\n",
    "            all_context.append(response.get('answer', []))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    [!] Search failed for query '{query}': {e}\")\n",
    "\n",
    "    # 5. Join the collected text into a single, comprehensive context string\n",
    "    job_role_context = \"\\n\\n---\\n\\n\".join(all_context)\n",
    "    \n",
    "    if not job_role_context:\n",
    "        print(\"--- âš ï¸ RESEARCH WARNING: No context was gathered. Downstream tasks may fail. ---\")\n",
    "    else:\n",
    "        print(f\"--- âœ… RESEARCH COMPLETE: Successfully gathered context for '{job_role}'. ---\")\n",
    "    \n",
    "    # 6. Update the state with the new context and increment the step counter\n",
    "    state['job_role_context'] = job_role_context\n",
    "    state['num_steps'] = state.get('num_steps', 0) + 1\n",
    "    \n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bed6bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def interview_chat(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Handles the interview interaction - asks question and collects user answer.\n",
    "    \"\"\"\n",
    "    qas = state.get(\"generated_qas\", [])\n",
    "    idx = state.get(\"current_q_index\", 0)\n",
    "\n",
    "    if idx >= len(qas):\n",
    "        print(\"âœ… Interview complete! All questions answered.\")\n",
    "        return state\n",
    "\n",
    "    question = qas[idx].get(\"question\", \"\")\n",
    "    print(f\"\\nQ{idx+1}: {question}\")\n",
    "\n",
    "    # Collect user answer in multi-line style. User types 'done' when finished.\n",
    "    lines = []\n",
    "    print(\"Type your answer. Type a blank line then confirm, or type 'done' on a new line to finish.\")\n",
    "    while True:\n",
    "        chunk = input()\n",
    "        if chunk.strip().lower() == \"done\":\n",
    "            break\n",
    "        # blank line = ask if done\n",
    "        if chunk.strip() == \"\":\n",
    "            yn = input(\"It looks like you pressed Enter. Are you done answering? (yes/no): \").strip().lower()\n",
    "            if yn in (\"yes\", \"y\"):\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        lines.append(chunk)\n",
    "\n",
    "    user_answer = \" \".join(lines).strip()\n",
    "    qas[idx][\"answer\"] = user_answer\n",
    "    state[\"generated_qas\"] = qas\n",
    "\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b345f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluator_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Evaluates the user's answer using an LLM and provides structured feedback.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- ðŸ“Š EVALUATING ANSWER ---\")\n",
    "\n",
    "    idx = state.get(\"current_q_index\", 0)\n",
    "    qas = state.get(\"generated_qas\", [])\n",
    "    \n",
    "    if idx >= len(qas):\n",
    "        print(\"[!] evaluator_node: current_q_index out of range.\")\n",
    "        return state\n",
    "\n",
    "    # Get required data from state\n",
    "    question = qas[idx].get(\"question\", \"\")\n",
    "    user_answer = qas[idx].get(\"answer\", \"\")\n",
    "    job_role_context = state.get(\"job_role_context\", \"\")\n",
    "    job_role = state.get(\"job_role\", \"\")\n",
    "\n",
    "    # Setup the output parser\n",
    "    parser = JsonOutputParser(pydantic_object=Evaluation)\n",
    "\n",
    "    # Create the prompt for the evaluator LLM\n",
    "    prompt_template = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are an expert technical interviewer and performance coach for the role of '{job_role}'.\n",
    "        Your task is to provide a fair, structured evaluation of the candidate's answer.\n",
    "\n",
    "        **Job Role Context:**\n",
    "        {job_context}\n",
    "\n",
    "        **Interview Question:**\n",
    "        {question}\n",
    "\n",
    "        **Candidate's Answer:**\n",
    "        {user_answer}\n",
    "        \n",
    "        **Instructions:**\n",
    "        Evaluate the answer based on clarity, technical accuracy, relevance to the role, and depth.\n",
    "        Provide your feedback in the required JSON format.\n",
    "\n",
    "        {format_instructions}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Create and invoke the evaluation chain\n",
    "    evaluator_chain = prompt_template | evaluator_llm | parser\n",
    "\n",
    "    try:\n",
    "        feedback = evaluator_chain.invoke({\n",
    "            \"job_role\": job_role,\n",
    "            \"job_context\": job_role_context,\n",
    "            \"question\": question,\n",
    "            \"user_answer\": user_answer,\n",
    "            \"format_instructions\": parser.get_format_instructions(),\n",
    "        })\n",
    "\n",
    "        # Ensure evaluation_feedback is a list\n",
    "        if \"evaluation_feedback\" not in state or not isinstance(state[\"evaluation_feedback\"], list):\n",
    "            state[\"evaluation_feedback\"] = []\n",
    "\n",
    "        # Expand list size if needed\n",
    "        while len(state[\"evaluation_feedback\"]) <= idx:\n",
    "            state[\"evaluation_feedback\"].append({})\n",
    "\n",
    "        # Store at index\n",
    "        state[\"evaluation_feedback\"][idx] = feedback\n",
    "\n",
    "        print(\"\\n--- ðŸ“ FEEDBACK ---\")\n",
    "        print(f\"Overall Score: {feedback['overall_score']}/10\")\n",
    "        print(f\"Justification: {feedback['score_justification']}\")\n",
    "        print(\"\\nStrengths:\")\n",
    "        for strength in feedback['strengths']:\n",
    "            print(f\"  - {strength}\")\n",
    "        print(\"\\nAreas for Improvement:\")\n",
    "        for improvement in feedback['areas_for_improvement']:\n",
    "            print(f\"  - {improvement}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error during evaluation: {e}\")\n",
    "        # Store error in evaluation_feedback\n",
    "        while len(state[\"evaluation_feedback\"]) <= idx:\n",
    "            state[\"evaluation_feedback\"].append({})\n",
    "        state[\"evaluation_feedback\"][idx] = {\"error\": \"Failed to generate evaluation.\"}\n",
    "        print(\"\\n--- ðŸ“ FEEDBACK ---\\nSorry, an error occurred while generating feedback.\")\n",
    "\n",
    "    # Ask user if they want to see an ideal response\n",
    "    choice = input(\"\\nDo you want to see an ideal response? (yes/no): \").strip().lower()\n",
    "    state[\"wants_ideal\"] = True if choice in (\"yes\", \"y\") else False\n",
    "\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2241a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_answer_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Generate ideal answer and advance to next question.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- âœ¨ GENERATING IDEAL ANSWER ---\")\n",
    "    idx = state.get(\"current_q_index\", 0)\n",
    "    qas = state.get(\"generated_qas\", [])\n",
    "\n",
    "    # Guard\n",
    "    if idx >= len(qas):\n",
    "        print(\"[!] generate_answer_node: current_q_index out of range.\")\n",
    "        return state\n",
    "\n",
    "    question = qas[idx].get(\"question\", \"\")\n",
    "    user_answer = qas[idx].get(\"answer\", \"\")\n",
    "    \n",
    "    # Get the feedback for this question\n",
    "    eval_feedback = state.get(\"evaluation_feedback\", [])\n",
    "    feedback_text = \"\"\n",
    "    if idx < len(eval_feedback) and eval_feedback[idx]:\n",
    "        fb = eval_feedback[idx]\n",
    "        if \"error\" not in fb:\n",
    "            feedback_text = f\"Score: {fb.get('overall_score', 'N/A')}/10. {fb.get('score_justification', '')}\"\n",
    "        \n",
    "    job_context = state.get(\"job_role_context\", \"\")\n",
    "\n",
    "    # Build prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert interviewer & answer coach.\n",
    "    Question: {question}\n",
    "    User's answer: {user_answer}\n",
    "    Evaluation feedback: {feedback_text}\n",
    "    Job context: {job_context}\n",
    "\n",
    "    Generate a concise, well-structured IDEAL answer to the question. Return only the answer text.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = generator_llm.invoke([HumanMessage(content=prompt)])\n",
    "        ideal_answer = getattr(response, \"content\", str(response)).strip()\n",
    "    except Exception as e:\n",
    "        ideal_answer = f\"[Generation failed: {e}]\"\n",
    "\n",
    "    # Ensure ideal_answers list exists and matches length of qas\n",
    "    ideal_answers = state.get(\"ideal_answers\", [])\n",
    "    if len(ideal_answers) < len(qas):\n",
    "        # initialize/extend to match number of questions\n",
    "        ideal_answers = [\"\"] * len(qas)\n",
    "\n",
    "    ideal_answers[idx] = ideal_answer\n",
    "    state[\"ideal_answers\"] = ideal_answers\n",
    "\n",
    "    # Print the generated ideal answer for user\n",
    "    print(f\"\\n--- ðŸ”Ž IDEAL ANSWER for Q{idx+1} ---\\n{ideal_answer}\\n\")\n",
    "\n",
    "    # Reset wants_ideal and advance to next question\n",
    "    state[\"wants_ideal\"] = False\n",
    "    state[\"current_q_index\"] = idx + 1\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49a2ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_next(state: GraphState) -> str:\n",
    "    \"\"\"\n",
    "    Conditional edge decision function.\n",
    "    \"\"\"\n",
    "    # if user requested ideal answer, go there\n",
    "    if state.get(\"wants_ideal\", False):\n",
    "        return \"generate_answer\"\n",
    "\n",
    "    # if we've finished all questions, end\n",
    "    if state.get(\"current_q_index\", 0) >= len(state.get(\"generated_qas\", [])):\n",
    "        return END\n",
    "\n",
    "    # otherwise, continue interviewing\n",
    "    return \"interview_chat\"\n",
    "\n",
    "\n",
    "def advance_question(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Simple node to advance to next question when user doesn't want ideal answer.\n",
    "    \"\"\"\n",
    "    idx = state.get(\"current_q_index\", 0)\n",
    "    state[\"current_q_index\"] = idx + 1\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00a665",
   "metadata": {},
   "source": [
    "### COMPILE THE GRAPH ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dceee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "    # Add nodes\n",
    "workflow.add_node(\"research\", research_node)\n",
    "workflow.add_node(\"generator\", generator_node)\n",
    "workflow.add_node(\"interview_chat\", interview_chat)\n",
    "workflow.add_node(\"evaluator\", evaluator_node)\n",
    "workflow.add_node(\"generate_answer\", generate_answer_node)\n",
    "workflow.add_node(\"advance_question\", advance_question)\n",
    "\n",
    "# Linear setup: START â†’ research â†’ generator â†’ interview_chat\n",
    "workflow.add_edge(START, \"research\")\n",
    "workflow.add_edge(\"research\", \"generator\")\n",
    "workflow.add_edge(\"generator\", \"interview_chat\")\n",
    "\n",
    "# Flow: interview_chat â†’ evaluator â†’ conditional next step\n",
    "workflow.add_edge(\"interview_chat\", \"evaluator\")\n",
    "\n",
    "# Conditional edge: evaluator â†’ generate_answer / advance_question\n",
    "workflow.add_conditional_edges(\n",
    "        \"evaluator\",\n",
    "        decide_next,\n",
    "        {\n",
    "            \"generate_answer\": \"generate_answer\",\n",
    "            \"interview_chat\": \"advance_question\",  # Go to advance_question first\n",
    "            END: END,\n",
    "        },\n",
    ")\n",
    "\n",
    "# After advancing question â†’ back to interview_chat\n",
    "workflow.add_edge(\"advance_question\", \"interview_chat\")\n",
    "\n",
    "# After generating ideal answer â†’ back to interview_chat\n",
    "workflow.add_edge(\"generate_answer\", \"interview_chat\")\n",
    "\n",
    "    # Compile\n",
    "workflow = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94716b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example initial state\n",
    "init_state: GraphState = {\n",
    "    \"job_role\": \"Data Scientist\",\n",
    "    \"job_role_context\": \"\",\n",
    "    \"candidate_data\": [],\n",
    "    \"generated_qas\": [],\n",
    "    \"evaluation_feedback\": [],\n",
    "    \"current_q_index\": 0,\n",
    "    \"wants_ideal\": False,\n",
    "    \"ideal_answers\": [],\n",
    "    \"num_steps\": 0,\n",
    "    \"max_iterations\": 10,\n",
    "}\n",
    "\n",
    "# Run the graph\n",
    "if __name__ == \"__main__\":\n",
    "    final_state = app.invoke(init_state)\n",
    "    print(\"\\n--- FINAL STATE ---\")\n",
    "    print(\"current_q_index:\", final_state[\"current_q_index\"])\n",
    "    print(\"Number of questions answered:\", len([qa for qa in final_state[\"generated_qas\"] if qa.get(\"answer\", \"\")]))\n",
    "    print(\"Number of evaluations:\", len(final_state[\"evaluation_feedback\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume_sel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
